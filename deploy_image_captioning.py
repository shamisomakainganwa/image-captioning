# -*- coding: utf-8 -*-
"""Deploy Image Captioning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MJezUiWoqwKosexuk7LUkGDzgFymv5-2
"""
#import libraries
import os
import streamlit as st
import pickle
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

from keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
import streamlit as st

# Add code for handling user uploads and interacting with the deployed model
def main():
    st.title('Video Captioning App')

    # Add code for handling user uploads and interacting with the deployed model

if __name__ == '__main__':
    main()

pip install opencv-python

import streamlit as st
import cv2

def main():
    st.title('Video Captioning App')

    # File Upload
    video_file = st.file_uploader('Upload Video', type=['mp4'])

    if video_file is not None:
        # Handle video upload and interact with the deployed model
        frames = split_video_into_frames(video_file)

        # Generate text descriptions for the frames
        captions = generate_captions(frames)

        # Display the video frames and their respective captions
        for frame, caption in zip(frames, captions):
            st.image(frame, caption=caption)
            st.text(caption)

def split_video_into_frames(video_file):
    frames = []
    video_bytes = video_file.read()

    # Convert video bytes to OpenCV VideoCapture object
    video_capture = cv2.VideoCapture(video_bytes)

    while True:
        # Read the next frame from the video
        ret, frame = video_capture.read()

        if not ret:
            break
            # Convert the frame to RGB format
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # Add the frame to the list of frames
        frames.append(frame_rgb)

    # Release the video capture object
    video_capture.release()

    return frames

def generate_captions(frames):
    captions = []
    # Implement the logic to generate captions for each frame using your deployed model
    # Store the captions in the captions list

    return captions

if __name__ == '__main__':
    main()
